# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fPshi1Xoy1XwS0tOZds7thtnGKvzaUtI
"""

import pandas as pd #import the library
df = pd.read_csv('Sleep_health_and_lifestyle_dataset.csv') #import the data set
df.head() # show the first 5 rows in the data set

#Inspect the data

df.info() #Find missing values/non values

df.describe() #statical summery(min,max,mean)

# Target variable(Y) is Sleep_Quality
# Features are Gender,	Age,	Occupation,	Sleep Duration,	Quality of Sleep,	Physical Activity, Level	Stress, Level	BMI, Category,	Blood Pressure,	Heart Rate,	Daily Steps,	Sleep Disorder

#Data Preprocessing (Crucial Step)
#Lable data(Lable encoder)

from sklearn.preprocessing import LabelEncoder

# 1. Split Blood Pressure into two columns
df[['Systolic', 'Diastolic']] = df['Blood Pressure'].str.split('/', expand=True).astype(int)

# 2. Convert text columns to numbers (Label Encoding)
le = LabelEncoder()
categorical_cols = ['Gender', 'Occupation', 'BMI Category', 'Sleep Disorder']

for col in categorical_cols:
    df[col] = le.fit_transform(df[col].astype(str))

# 3. Define Features (X) and Target (y)
# Note:  removed 'Quality of Sleep' from X because it is your target
features = ['Gender', 'Age', 'Occupation', 'Sleep Duration', 'Physical Activity Level',
            'Stress Level', 'BMI Category', 'Heart Rate', 'Daily Steps',
            'Sleep Disorder', 'Systolic', 'Diastolic']

X = df[features]
y = df['Quality of Sleep'] # This is your target variable

print("Data is now numerical and ready!")
X.head()

#Feature Scaling (Very Important for SVM)

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Split into Training and Testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

#Train and Evaluate Random Forest
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score

# Initialize the model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train) # We use unscaled data for Random Forest

# Make predictions
rf_preds = rf_model.predict(X_test)

# Check results
print("--- Random Forest Performance ---")
print(f"Accuracy: {accuracy_score(y_test, rf_preds) * 100:.2f}%")
print(classification_report(y_test, rf_preds))

#Train and Evaluate SVM
from sklearn.svm import SVC

# Initialize the model
svm_model = SVC(kernel='linear', random_state=42) # You can try 'rbf' kernel later too!

# Train the model (Using SCALED data)
svm_model.fit(X_train_scaled, y_train)

# Make predictions
svm_preds = svm_model.predict(X_test_scaled)

# Check results
print("--- SVM Performance ---")
print(f"Accuracy: {accuracy_score(y_test, svm_preds) * 100:.2f}%")
print(classification_report(y_test, svm_preds))

#Removing Biased and Irrelevant Features
#To solve the "Cheat Sheet" issue (Stress Level) and the "Identification" issue (Person ID), we simply drop those columns from your features variable X

# List of features to REMOVE
to_drop = ['Person ID', 'Stress Level', 'Quality of Sleep']

# Create a new X without these columns
X_refined = df.drop(columns=to_drop)
y = df['Quality of Sleep']

print("Refined Features:", X_refined.columns.tolist())

#Fix: Data Redundancy and Overfitting
# Create a temporary dataframe to drop duplicates
# We drop duplicates based on everything except 'Person ID'
df_cleaned = df.drop(columns=['Person ID']).drop_duplicates()

print(f"Rows before: {len(df)} | Rows after removing duplicates: {len(df_cleaned)}")

# Now update your X and y based on this cleaned data
y = df_cleaned['Quality of Sleep']
X = df_cleaned.drop(columns=['Quality of Sleep', 'Stress Level'])

from sklearn.preprocessing import LabelEncoder

# 1. Split Blood Pressure into 'Systolic' and 'Diastolic' numbers
# We use .str.split('/') to break '120/80' into two parts
df_cleaned[['Systolic', 'Diastolic']] = df_cleaned['Blood Pressure'].str.split('/', expand=True).astype(int)

# 2. Drop the original 'Blood Pressure' column because we don't need the string anymore
df_cleaned = df_cleaned.drop(columns=['Blood Pressure'])

# 3. Convert other text columns (Gender, Occupation, etc.) to numbers
le = LabelEncoder()
categorical_cols = ['Gender', 'Occupation', 'BMI Category', 'Sleep Disorder']

for col in categorical_cols:
    df_cleaned[col] = le.fit_transform(df_cleaned[col].astype(str))

# 4. Now re-define X and y with the fixed data
y = df_cleaned['Quality of Sleep']
X = df_cleaned.drop(columns=['Quality of Sleep', 'Stress Level'])

print("Data successfully converted to numbers!")
print(f"New columns: {X.columns.tolist()}")

#Split and Scale (The "Final Prep")
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# 1. Split the data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 2. Scale the data (Important for SVM!)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

print("Step 1 Complete: Data is split and scaled.")

#Train and Compare (The "Actual ML")
#Random Forest and SVM and see which one predicts sleep quality better

from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# 1. Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train) # RF works fine with unscaled data
rf_acc = accuracy_score(y_test, rf.predict(X_test))

# 2. SVM
svm = SVC(kernel='linear')
svm.fit(X_train_scaled, y_train) # SVM MUST use scaled data
svm_acc = accuracy_score(y_test, svm.predict(X_test_scaled))

print(f"Random Forest Accuracy: {rf_acc * 100:.2f}%")
print(f"SVM Accuracy: {svm_acc * 100:.2f}%")

#Visualize the Result (The "Engineering Report")

import matplotlib.pyplot as plt

model_names = ['Random Forest', 'SVM']
accuracies = [rf_acc * 100, svm_acc * 100]

plt.figure(figsize=(8, 5))
plt.bar(model_names, accuracies, color=['#3498db', '#e74c3c'])
plt.ylim(0, 100)
plt.ylabel('Accuracy (%)')
plt.title('Comparison of Sleep Quality Prediction Models')
for i, v in enumerate(accuracies):
    plt.text(i, v + 2, f"{v:.2f}%", ha='center', fontweight='bold')
plt.show()

#save your model to a file
import joblib

# Save the Random Forest model to a file
joblib.dump(rf, 'sleep_quality_model.pkl')
joblib.dump(scaler, 'scaler.pkl') # Save the scaler too!

print("Model saved! You can now download 'sleep_quality_model.pkl' from the Colab files tab.")

#Mapping Numbers to "Poor, Fair, Good, Excellent"
def categorize_sleep(score):
    if score <= 5: return 'Poor'
    elif score <= 6: return 'Fair'
    elif score <= 8: return 'Good'
    else: return 'Excellent'

# Apply categorization
df_cleaned['Sleep_Category'] = df_cleaned['Quality of Sleep'].apply(categorize_sleep)

# Check the new distribution
print(df_cleaned['Sleep_Category'].value_counts())

#Identify "Key Factors" (Feature Importance)

import matplotlib.pyplot as plt
import numpy as np

# Get feature importances from your trained Random Forest
importances = rf.feature_importances_
feature_names = X.columns
indices = np.argsort(importances)

plt.figure(figsize=(10, 6))
plt.title('Key Factors Affecting Sleep Quality')
plt.barh(range(len(indices)), importances[indices], color='teal', align='center')
plt.yticks(range(len(indices)), [feature_names[i] for i in indices])
plt.xlabel('Importance Score (Higher is more influential)')
plt.show()